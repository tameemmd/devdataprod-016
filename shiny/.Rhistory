data(concrete)
library(caret)
library(Hmisc)
library(plyr)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
splitOn <- cut2(training$Age, g = 4)
splitOn <- mapvalues(splitOn,
from = levels(factor(splitOn)),
to = c("red", "blue", "yellow", "green"))
par(mfrow = c(1,2))
# automatically includes index of samples
plot(training$CompressiveStrength, col = splitOn)
splitOn2 <- cut2(training$FlyAsh, g = 3)
splitOn2 <- mapvalues(splitOn2,
from = levels(factor(splitOn2)),
to = c("red", "blue", "yellow"))
# automatically includes index of samples
plot(training$CompressiveStrength, col = splitOn2)
par(mfrow = c(1,1))
# Answer: There is a step-like pattern in the plot of outcome versus index in
# the training set that isn't explained by any of the predictor variables so
# there may be a variable missing.
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
summary(concrete)
hist(training$Superplasticizer)
hist(training[,5])
preProc <- preProcess(log10(training[,-5]+1, pethod=pca, pcaComp=2))
preProc <- preProcess(log10(training[,-5]+1, method="pca", pcaComp=2))
preProc <- preProcess(log10(training[,-5]+1), method="pca", pcaComp=2)
trainPC <- predict(preProc, log10(training[, -5] + 1))
plot(trainPC[,1], trainPC[,2])
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
ggplot(data = training, aes(x = Superplasticizer)) + geom_histogram() + theme_bw()
# Answer: The log transform does not reduce the skewness of the non-zero values of SuperPlasticizer
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
par(mfrow = c(1,2))
ggplot(data = training, aes(x = Superplasticizer)) + geom_histogram() + theme_bw()
ggplot(data = training, aes(x = log10(Superplasticizer)) + geom_histogram() + theme_bw()
par(mfrow = c(1,1))
# Answer: The log transform does not reduce the skewness of the non-zero values of SuperPlasticizer
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
par(mfrow = c(1,2))
ggplot(data = training, aes(x = Superplasticizer)) + geom_histogram() + theme_bw()
ggplot(data = training, aes(x = log10(Superplasticizer))) + geom_histogram() + theme_bw()
par(mfrow = c(1,1))
# Answer: The log transform does not reduce the skewness of the non-zero values of SuperPlasticizer
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
par(mfrow = c(2,1))
ggplot(data = training, aes(x = Superplasticizer)) + geom_histogram() + theme_bw()
ggplot(data = training, aes(x = log10(Superplasticizer))) + geom_histogram() + theme_bw()
par(mfrow = c(1,1))
# Answer: The log transform does not reduce the skewness of the non-zero values of SuperPlasticizer
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
?AlzheimerDisease
summary(AlzheimerDisease)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
predictors
summary(predictors)
cnames(Predictor)
cNames(Predictor)
cName(Predictor)
cname(Predictor)
colnames(Predictor)
adData
colnames
colnames(adData)
adData[,"IL" %IN% colnames(adData))
adData[,"IL" %IN% colnames(adData)]
adData[,grep("$IL", colnames(adData)]
adData[,grep("$IL", colnames(adData))]
grep("$IL", colnames(adData))
grep("$IL", colnames(adData))
colnames(adData)
grep("IL", colnames(adData))
adData[,grep("IL", colnames(adData))]
preProc <- preProcess(log10(training[,-grep("IL", colnames(adData))]+1), method="pca", pcaComp=2)
training[,-grep("IL", colnames(adData))]
preProc <- preProcess(log10(training[,-grep("IL", colnames(adData))]+1), method="pca", pcaComp=2)
log10(training[,-grep("IL", colnames(adData))]+1)
preProc <- preProcess(log10(training[,grep("IL", colnames(adData))]+1), method="pca", pcaComp=2)
?preProcess
IL_variables <- grep("^IL", names(training), value = TRUE)
preProc <- preProcess(training[, IL_variables], method = "pca", thresh = 0.9)
preProc <- preProcess(training[,grep("IL", colnames(adData))], method="pca", pcaComp=2)
preProc
preProc2 <- preProcess(training[, IL_variables], method = "pca", thresh = 0.9)
preProc2
preProc2 <- preProcess(training[, IL_variables], method = "pca", thresh = 0.8)
preProc2
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
IL_variables <- grep("^IL", names(training), value = TRUE)
head(training)
modFit <- train(diagnosis ~ training[, IL_variables], method="glm", data=training)
training[, IL_variables]
modFit <- train(diagnosis ~ training[, IL_variables], method="glm", data=training)
featurePlot(x=training[, IL_variables], y=training$diagnosis, plot="pairs")
modFit <- train(diagnosis ~ IL_variables, method="glm", data=training)
modFit <- train(diagnosis ~ IL_variables, method="lm", data=training)
modFit <- train(diagnosis ~ training[,IL_variables], method="lm", data=training)
?train
IL_variables
chkData <- adData[,c(diagnosis, IL_variables)]
chkData <- adData[,c("diagnosis", IL_variables)]
modFit <- train(diagnosis ~ ., method="glm", data=chkData)
finMod <- modFit$finalModel
finMod
print(finMod)
modelFit1 <- train(chkData$diagnosis ~ ., method = "glm", preProcess="pca", data=chkData)
modelFit2 <- train(chkData$diagnosis ~ ., method = "glm", data=chkData)
modelFit1
modelFit2
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
predictors
diagnosis
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
## grep the predictors starting with 'IL'
IL_str <- grep("^IL", colnames(training), value = TRUE)
## make a subset of these predictors
predictors_IL <- predictors[, IL_str]
# create a new DF of predictors and diagnosis
df <- data.frame(diagnosis, predictors_IL)
# create a training and testing set from this DF
inTrain = createDataPartition(df$diagnosis, p = 3/4)[[1]]
training = df[inTrain, ]
testing = df[-inTrain, ]
## train the data using the first method
modelFit <- train(diagnosis ~ ., method = "glm", data = training)
predictions <- predict(modelFit, newdata = testing)
## get the confusion matrix for the first method
C1 <- confusionMatrix(predictions, testing$diagnosis)
print(C1)
A1 <- C1$overall[1]
## do similar steps with PCA
modelFit <- train(training$diagnosis ~ ., method = "glm", data = training,
preProcess = "pca",
Control = trainControl(preProcOptions = list(thresh = 0.8)))
C2 <- confusionMatrix(testing$diagnosis, predict(modelFit, testing))
print(C2)
set.seed(3433)
## grep the predictors starting with 'IL'
IL_str <- grep("^IL", colnames(training), value = TRUE)
## make a subset of these predictors
predictors_IL <- predictors[, IL_str]
# create a new DF of predictors and diagnosis
df <- data.frame(diagnosis, predictors_IL)
# create a training and testing set from this DF
inTrain = createDataPartition(df$diagnosis, p = 3/4)[[1]]
training = df[inTrain, ]
testing = df[-inTrain, ]
## train the data using the first method
modelFit <- train(diagnosis ~ ., method = "glm", data = training)
predictions <- predict(modelFit, newdata = testing)
## get the confusion matrix for the first method
C1 <- confusionMatrix(predictions, testing$diagnosis)
A1 <- C1$overall[1]
## do similar steps with PCA
modelFit <- train(training$diagnosis ~ ., method = "glm", data = training,
preProcess = "pca",
Control = trainControl(preProcOptions = list(thresh = 0.8)))
C2 <- confusionMatrix(testing$diagnosis, predict(modelFit, testing))
print(C1)
print(C2)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(3433)
## grep the predictors starting with 'IL'
IL_str <- grep("^IL", colnames(training), value = TRUE)
## make a subset of these predictors
predictors_IL <- predictors[, IL_str]
# create a new DF of predictors and diagnosis
df <- data.frame(diagnosis, predictors_IL)
# create a training and testing set from this DF
inTrain = createDataPartition(df$diagnosis, p = 3/4)[[1]]
training = df[inTrain, ]
testing = df[-inTrain, ]
## train the data using the first method
modelFit <- train(diagnosis ~ ., method = "glm", data = training,
Control = trainControl(preProcOptions = list(thresh = 0.8)))
predictions <- predict(modelFit, newdata = testing)
## get the confusion matrix for the first method
C1 <- confusionMatrix(predictions, testing$diagnosis)
A1 <- C1$overall[1]
## do similar steps with PCA
modelFit <- train(training$diagnosis ~ ., method = "glm", data = training,
preProcess = "pca",
Control = trainControl(preProcOptions = list(thresh = 0.8)))
C2 <- confusionMatrix(testing$diagnosis, predict(modelFit, testing))
print(C1)
print(C2)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
## grep the predictors starting with 'IL'
IL_str <- grep("^IL", colnames(training), value = TRUE)
## make a subset of these predictors
predictors_IL <- predictors[, IL_str]
# create a new DF of predictors and diagnosis
df <- data.frame(diagnosis, predictors_IL)
# create a training and testing set from this DF
inTrain = createDataPartition(df$diagnosis, p = 3/4)[[1]]
training = df[inTrain, ]
testing = df[-inTrain, ]
## train the data using the first method
modelFit <- train(diagnosis ~ ., method = "glm", data = training)
predictions <- predict(modelFit, newdata = testing)
## get the confusion matrix for the first method
C1 <- confusionMatrix(predictions, testing$diagnosis)
print(C1)
A1 <- C1$overall[1]
## do similar steps with PCA
modelFit <- train(training$diagnosis ~ ., method = "glm", data = training,
preProcess = "pca",
Control = trainControl(preProcOptions = list(thresh = 0.8)))
C2 <- confusionMatrix(testing$diagnosis, predict(modelFit, testing))
print(C2)
?randomForest
library(ElemStatLearn)
library(randomForest)
library(caret)
data(vowel.train)
data(vowel.test)
vowel.train$y <- factor(vowel.train$y)
vowel.test$y <- factor(vowel.test$y)
set.seed(33833)
?randomForest
a <- randomForest(y ~ ., data = vowel.train, importance = TRUE)
b <- varImp(a)
order(b)
library(ElemStatLearn)
library(randomForest)
library(caret)
data(vowel.train)
data(vowel.test)
vowel.train$y <- factor(vowel.train$y)
vowel.test$y <- factor(vowel.test$y)
set.seed(33833)
# Fit a random forest predictor relating the factor variable y to the remaining variables.
a <- randomForest(y ~ ., data = vowel.test, importance = FALSE)
b <- varImp(a)
order(b)
library(ElemStatLearn)
library(randomForest)
library(caret)
data(vowel.train)
data(vowel.test)
vowel.train$y <- factor(vowel.train$y)
vowel.test$y <- factor(vowel.test$y)
set.seed(33833)
# Fit a random forest predictor relating the factor variable y to the remaining variables.
a <- randomForest(y ~ ., data = vowel.train, prox=TRUE)
b <- varImp(a)
order(b)
library(ElemStatLearn)
library(randomForest)
library(caret)
data(vowel.train)
data(vowel.test)
vowel.train$y <- factor(vowel.train$y)
vowel.test$y <- factor(vowel.test$y)
set.seed(33833)
# Fit a random forest predictor relating the factor variable y to the remaining variables.
a <- randomForest(y ~ ., data = vowel.train, prox=TRUE)
b <- varImp(a)
order(b, decreasing = FALSE)
library(ElemStatLearn)
library(randomForest)
library(caret)
data(vowel.train)
data(vowel.test)
vowel.train$y <- factor(vowel.train$y)
vowel.test$y <- factor(vowel.test$y)
set.seed(33833)
# Fit a random forest predictor relating the factor variable y to the remaining variables.
a <- randomForest(y ~ ., data = vowel.train, prox=TRUE)
b <- varImp(a)
order(b, decreasing = TRUE)
library(ElemStatLearn)
library(randomForest)
library(caret)
data(vowel.train)
data(vowel.test)
# Set the variable y to be a factor variable in both the training and test set.
# Then set the seed to 33833. Fit (1) a random forest predictor relating the
# factor variable y to the remaining variables and (2) a boosted predictor using
# the "gbm" method. Fit these both with the train() command in the caret package.
vowel.train$y <- factor(vowel.train$y)
vowel.test$y <- factor(vowel.test$y)
set.seed(33833)
# create models
fit1 <- train(y ~ ., data = vowel.train, method = "rf", trControl = trainControl(number = 4))
fit2 <- train(y ~ ., data = vowel.train, method = "gbm")
library(ElemStatLearn)
library(randomForest)
library(caret)
data(vowel.train)
data(vowel.test)
# Set the variable y to be a factor variable in both the training and test set.
# Then set the seed to 33833. Fit (1) a random forest predictor relating the
# factor variable y to the remaining variables and (2) a boosted predictor using
# the "gbm" method. Fit these both with the train() command in the caret package.
vowel.train$y <- factor(vowel.train$y)
vowel.test$y <- factor(vowel.test$y)
set.seed(33833)
# create models
fit1 <- train(y ~ ., data = vowel.train, method = "rf", trControl = trainControl(number = 4))
fit2 <- train(y ~ ., data = vowel.train, method = "gbm")
# predict test
predict1 <- predict(fit1, newdata = vowel.test)
predict2 <- predict(fit2, newdata = vowel.test)
DF_combined <- data.frame(predict1, predict2, y = vowel.test$y)
fit_combined <- train(y ~ ., data = DF_combined, method = "gam")
predict3 <- predict(fit_combined, newdata = vowel.test)
c1 <- confusionMatrix(predict1, vowel.test$y)
c2 <- confusionMatrix(predict2, vowel.test$y)
c3 <- confusionMatrix(predict3, DF_combined$y)
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
# Set the seed to 62433 and predict diagnosis with all the other variables using
# a random forest ("rf"), boosted trees ("gbm") and linear discriminant analysis
# ("lda") model. Stack the predictions together using random forests ("rf").
# What is the resulting accuracy on the test set? Is it better or worse than
# each of the individual predictions?
set.seed(62433)
# create models
fit1 <- train(diagnosis ~ ., data = training, method = "rf", trControl = trainControl(number = 4))
fit2 <- train(diagnosis ~ ., data = training, method = "gbm")
fit3 <- train(diagnosis ~ ., data = training, method = "lda")
# predict test
predict1 <- predict(fit1, newdata = testing)
predict2 <- predict(fit2, newdata = testing)
predict3 <- predict(fit3, newdata = testing)
# combine predictions
DF_combined <- data.frame(predict1, predict2, predict3, diagnosis = testing$diagnosis) # training$diagnosis?
fit_combined <- train(diagnosis ~ ., data = DF_combined, method = "rf")
predict4 <- predict(fit_combined, newdata = testing)
# confusion matrixes
c1 <- confusionMatrix(predict1, testing$diagnosis)
c2 <- confusionMatrix(predict2, testing$diagnosis)
c3 <- confusionMatrix(predict3, testing$diagnosis)
c4 <- confusionMatrix(predict4, testing$diagnosis)
print(paste(c1$overall[1], c2$overall[1], c3$overall[1], c4$overall[1]))
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
# Set the seed to 325 and fit a support vector machine using the e1071 package
# to predict Compressive Strength using the default settings. Predict on the
# testing set. What is the RMSE?
set.seed(325)
library(e1071)
library(caret)
fit <- train(CompressiveStrength ~ ., data = training, method = "svmRadial")
prediction <- predict(fit, testing)
accuracy(prediction, testing$CompressiveStrength)
?accuracy
library(forcast)
install.packages("forcast")
install.packages("forcast")
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
# Set the seed to 325 and fit a support vector machine using the e1071 package
# to predict Compressive Strength using the default settings. Predict on the
# testing set. What is the RMSE?
set.seed(325)
library(e1071)
library(caret)
fit <- train(CompressiveStrength ~ ., data = training, method = "svmRadial")
prediction <- predict(fit, testing)
accuracy(prediction, testing$CompressiveStrength)
library(forecast)
accuracy(prediction, testing$CompressiveStrength)
Tip = c(5,17,11,8,14,5)
Tip = c(5,17,11,8,14,5)
mean(Tip)
median(Tip)
summary(Tip)
hist(Tip)
plot(Tip)
plot(Tip)
abline(mean9Tip)
abline(mean(Tip))
abline(1:6,mean(Tip))
Bill = c(34,108,64,88,99,51)
d <- lm(Tip ~ Bill)
d
plot(d)
d
plot(d)
d$coef
predict(d, 200)
predict(d, data=c(200)
)
df = data.frame(Bill=200)
predict(d, df
)
scores = c(82,93,61,74,69,70,53,71,62,85,94,78,66,71,64,73,87,91,56,78,87)
year = c(rep("Yr1", 7),rep("Yr2", 7),rep("Yr3", 7))
data=data.frame(year,scores)
plot(scores ~ year)
plot(scores ~ year, data=data)
aov(scores ~ year, data=data)
results <- aov(scores ~ year, data=data)
summary(results)
colSums
class(colSums)
methods(colSums)
methods("colSums")
showMethods("colSums")
showMethods("show")
getMethod("lmList","show")
getMethod("genericFunction","show")
getMethod("ANY","show")
?getMethod
getMethod("show", "ANY")
showMethod("show", "ANY")
showMethods("show")
showMethod("show", "lmList")
showMethod("show", "color")
showMethods("show")
getMethod("show", "abIndex")
install.packages("E:\Downloads\DDPQuiz3_1.0.zip", repos = NULL)
install.packages("E:/Downloads/DDPQuiz3_1.0.zip", repos = NULL)
library(DDPQuiz3_1_0)
library(DDPQuiz3)
setwd("E:/Data Science - John Hopkins University/09. Developing Data Products/Project/devdataprod-016/shiny")
libray(shiny)
library(shiny)
runApp()
